# EM Algorithm - Gaussian Mixture Model Implementation

Implementation of the Expectation-Maximization (EM) algorithm for clustering using Gaussian Mixture Models (GMM).

## ğŸ“‹ Project Overview

This project implements the EM algorithm with the following steps:
1. **Data Generation** - Creates synthetic data from 3 Gaussian distributions
2. **Ground Truth Visualization** - Plots the true clusters
3. **Parameter Initialization** - Randomly initializes means, covariances, and weights
4. **Initial State Visualization** - Shows the initial parameter guesses
5. **E-Step (Expectation)** - Computes posterior probabilities using Bayes theorem
6. **M-Step (Maximization)** - Updates parameters using weighted averages
7. **Convergence Loop** - Iterates until log-likelihood converges

## ğŸš€ Features

- âœ… Complete EM algorithm implementation from scratch
- âœ… Interactive visualizations with matplotlib
- âœ… Convergence monitoring with log-likelihood tracking
- âœ… Gaussian ellipse visualization
- âœ… Accuracy calculation with Hungarian algorithm
- âœ… All plots automatically saved to `outputs/` folder

## ğŸ“¦ Requirements

```bash
pip install numpy matplotlib scipy
```

## ğŸ’» Usage

```bash
python em_algorithm_project.py
```

The script will:
- Generate 300 data points from 3 clusters
- Run the EM algorithm until convergence
- Display interactive plots (close each to continue)
- Save all plots to `outputs/` folder

## ğŸ“Š Output

The program generates 5 visualizations:
1. `1_ground_truth.png` - True cluster assignments
2. `2_initial_state.png` - Random initial parameters
3. `3_convergence.png` - Log-likelihood over iterations
4. `4_final_clusters.png` - Final clustering result with Gaussian ellipses
5. `5_comparison.png` - Side-by-side comparison of ground truth vs EM result

## ğŸ¯ Results

- **Convergence**: Typically converges in ~30 iterations
- **Accuracy**: Achieves ~98% clustering accuracy
- **Log-Likelihood**: Monitors convergence with threshold of 1e-4

## ğŸ“ Algorithm Details

### E-Step (Expectation)
Computes the responsibility of each cluster for each data point using Bayes theorem:

```
Î³(z_nk) = [Ï€_k * N(x_n|Î¼_k, Î£_k)] / Î£_j [Ï€_j * N(x_n|Î¼_j, Î£_j)]
```

### M-Step (Maximization)
Updates parameters using weighted averages:

```
Î¼_k = Î£_n [Î³(z_nk) * x_n] / Î£_n Î³(z_nk)
Î£_k = Î£_n [Î³(z_nk) * (x_n - Î¼_k)(x_n - Î¼_k)áµ€] / Î£_n Î³(z_nk)
Ï€_k = Î£_n Î³(z_nk) / N
```

## ğŸ”§ Configuration

You can modify parameters in the `main()` function:
- `n_samples_per_cluster`: Number of points per cluster (default: 100)
- `K`: Number of clusters (default: 3)
- `max_iter`: Maximum iterations (default: 100)
- `tol`: Convergence threshold (default: 1e-4)

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ em_algorithm_project.py    # Main implementation
â”œâ”€â”€ outputs/                    # Generated plots
â”‚   â”œâ”€â”€ 1_ground_truth.png
â”‚   â”œâ”€â”€ 2_initial_state.png
â”‚   â”œâ”€â”€ 3_convergence.png
â”‚   â”œâ”€â”€ 4_final_clusters.png
â”‚   â””â”€â”€ 5_comparison.png
â””â”€â”€ README.md
```

## ğŸ“ Educational Purpose

This project was created as part of a Computer Vision course to understand:
- Unsupervised learning algorithms
- Expectation-Maximization framework
- Gaussian Mixture Models
- Soft clustering vs hard clustering
- Convergence analysis

## ğŸ“„ License

MIT License - Feel free to use for educational purposes.
